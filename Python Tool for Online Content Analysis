import requests
from bs4 import BeautifulSoup
import re

#Step 1: Data Extraction
# url = 'https://www.doctor.com/Dr-Alice-Urbankova?utm_source=doctor'
# your_path = r'C:\Users\brend\Documents\MSIE_Brendaliz\Spring2022\ININ6999\Python'
url = input('Enter your url: ')
your_path = str(input('Enter the path of where your files will be located: '))
req_data= requests.get(url)
file_name=url.split('/')
file_name=file_name[3]
file_name= file_name.split('?')
file_name = str(file_name[0]) + str('.txt')
req_data.status_code

req_data.text

soup= BeautifulSoup(req_data.text, 'html.parser')
divs= soup.findAll(class_='review-text')


with open(file_name, 'w') as file:
    for div in divs:
        review = div.p.next_sibling.next_sibling.string
        print(str(review))
        file.write(str(review))
        
#Step 2: Data Cleaning
import re
text = open(your_path  +'\\' + file_name, encoding="cp1252").read()
text=text.lower()
new_text = re.sub('[^a-zA-Z0-9\n\.]', ' ', text)
print(new_text)
open(file_name, 'w').write(new_text)
#Step 2.1: Stop Word Removal
import nltk
from nltk.corpus import stopwords
text = open(your_path  +'\\' + file_name, encoding="utf8").read()
words = text.split()
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
stop_words = set(stopwords.words('english'))
stop_words.update(['.',',','"',"'",'?','!',':',';','(',')','[',']','{','}','Dr','Dr.','dr','dr.'])
words = [word for word in words if not word in stop_words]

# Step 2.2: Lemmatizing
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
Lemmatized_tokens = [lemmatizer.lemmatize(word) for word in words]
Final_Corpus = Lemmatized_tokens
# print(Final_Corpus)

#Step 3:Word Frequency
from nltk import FreqDist
frequency_distribution = FreqDist(Final_Corpus)
# print(frequency_distribution)
top_10 = frequency_distribution.most_common(10)

#Step 3.1: Word Frequency Cleaning for Plotting
my_list_words = str(re.findall(r"('\w+')", str(top_10)))
my_list_numbers = str(re.findall('[0-9]+', str(top_10)))
my_list_words = my_list_words.replace("'","")
my_list_words = my_list_words.replace("[","")
my_list_words = my_list_words.replace("]","")
my_list_words = my_list_words.replace('"',"")
my_list_words = my_list_words.split(",")


my_list_numbers = my_list_numbers.replace("'","")
my_list_numbers = my_list_numbers.replace("[","")
my_list_numbers = my_list_numbers.replace("]","")
my_list_numbers = my_list_numbers.replace('"',"")
my_list_numbers = my_list_numbers.split(",")

numbers = [ ]
for item in my_list_numbers:
    item = int(item)
    numbers.append(item)
    
#Step 3.2: Bar Plot
import matplotlib.pyplot as plt

x_axis = my_list_words
y_axis = numbers

plt.bar(x_axis, y_axis)
plt.title('Word Frequency')
plt.xlabel('Most Frequent Words')
plt.ylabel('Word Frequency')
plt.xticks(rotation = 45)
plt.show()

#Step 4: Word Cloud Creation
from wordcloud import WordCloud

text = open(file_name,mode='r', encoding = 'utf-8').read()

wordcloud = WordCloud(
        background_color = 'black',
        stopwords = stop_words,
        height = 500,
        width = 500
    
)
wordcloud.generate(text)

#store to file
wordcloud.to_file(file_name + '.png')

